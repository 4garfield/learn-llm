{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0543977a-7813-48a4-8880-4242ea5e7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare env(shell command)\n",
    "\n",
    "# conda create -n rag python=3.10\n",
    "# conda activate rag\n",
    "# conda install jupyter \n",
    "# pip install ipykernel\n",
    "# python -m ipykernel install --user --name rag --display-name rag\n",
    "\n",
    "# pip install -U langchain langchain-community langchain-ollama langchain-chroma pymupdf gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e7711f4-c41d-4514-9e24-7007898eea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a8f652-3073-4a4b-8383-2f0dda9e7582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document using PyMuPDFLoader\n",
    "loader = PyMuPDFLoader(\"./docs/2005.11401.pdf\")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b0d7cf-1947-4cd2-abf5-1c2d57498289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document into smaller chunks\n",
    "# Adjust the chunk_size and chunk_overlap parameters to balance performance and retrieval quality.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f577bdc1-3421-47b3-ab03-8a7783f0914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ollama embeddings\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = chunks,\n",
    "    embedding = OllamaEmbeddings(model=\"bge-m3\"),\n",
    ")\n",
    "\n",
    "# Initialize retriever using Ollama embeddings for queries\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "908bcc15-5697-44a7-a884-1484a51f40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(question):\n",
    "    # Retrieve relevant documents\n",
    "    docs = retriever.invoke(question)\n",
    "    # Combine the retrieved content\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bc7a055-94b6-4b72-ae5d-a07eb7722b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_deepseek(question, context):\n",
    "    formatted_prompt = f\"Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n\\n\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\"user\", formatted_prompt)],\n",
    "    )\n",
    "    # Query local DeepSeek-R1 model\n",
    "    llm = ChatOllama(\n",
    "        model = \"deepseek-r1:14b\",\n",
    "        temperature = 0.5\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    return chain.invoke({\"context\": context, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20bc8425-2abd-4832-9e82-6ce1a04845a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve context and generate an answer using RAG\n",
    "def ask_question(question):\n",
    "    answer = query_deepseek(question, format_context(question))\n",
    "    return answer\n",
    "\n",
    "# Set up the Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn = ask_question,\n",
    "    inputs = \"text\",\n",
    "    outputs = \"text\",\n",
    "    title = \"RAG Chatbot: Foundations of LLMs\",\n",
    "    description= \"Ask any question about the Foundations of LLMs book. Powered by DeepSeek-R1.\"\n",
    ")\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c5e50-70b6-47bc-bfd8-0cb2b5c581a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
