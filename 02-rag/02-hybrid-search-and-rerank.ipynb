{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad55e11f-2a6c-401e-8c41-eca5672d0cdb",
   "metadata": {},
   "source": [
    "## Hybird Search & Rerank\n",
    "\n",
    "Reference:\n",
    "* [Advanced RAG Implementation using Hybrid Search and Reranking](https://medium.com/@nadikapoudel16/advanced-rag-implementation-using-hybrid-search-reranking-with-zephyr-alpha-llm-4340b55fef22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61edbc9a-bbdb-42b4-bc1c-69f528078d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U faiss-cpu rank_bm25\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "loader = PyMuPDFLoader(\n",
    "    file_path = \"./docs/2005.11401.pdf\",\n",
    "    mode = \"page\",\n",
    "    extract_tables = \"markdown\"\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "recursive_chunks = recursive_text_splitter.split_documents(documents)\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents = recursive_chunks,\n",
    "    embedding = OllamaEmbeddings(model=\"bge-m3\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85db336d-0b91-479e-8a52-677538d5889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what's key concepts of RAG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb3d0224-7fca-4b6b-a592-1d20bbc38e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n",
      "4.5\n",
      "Additional Results\n",
      "Generation Diversity\n",
      "Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
      "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n",
      "[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\n",
      "total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are\n",
      "more diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing\n",
      "any diversity-promoting decoding.\n",
      "Retrieval Ablations\n",
      "A key feature of RAG is learning to retrieve relevant information for the task.\n",
      "To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\n",
      "during training. As shown in Table 6, learned retrieval improves results for all tasks.\n",
      "two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way\n",
      "(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\n",
      "4\n",
      "Results\n",
      "4.1\n",
      "Open-domain Question Answering\n",
      "Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\n",
      "tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\n",
      "the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of\n",
      "\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\n",
      "without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s\n",
      "retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions\n",
      "and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-\n",
      "decisions and updating their world knowledge remain open research problems. Pre-\n",
      "trained models with a differentiable access mechanism to explicit non-parametric\n",
      "memory have so far been only investigated for extractive downstream tasks. We\n",
      "explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\n",
      "(RAG) — models which combine pre-trained parametric and non-parametric mem-\n",
      "ory for language generation. We introduce RAG models where the parametric\n",
      "memory is a pre-trained seq2seq model and the non-parametric memory is a dense\n",
      "vector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\n",
      "pare two RAG formulations, one which conditions on the same retrieved passages\n",
      "across the whole generated sequence, and another which can use different passages\n",
      "per token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\n",
      "intensive NLP tasks and set the state of the art on three open domain QA tasks,\n",
      "Memory-based Architectures\n",
      "Our document index can be seen as a large external memory for\n",
      "neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\n",
      "to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\n",
      "work. Other work improves the ability of dialog models to generate factual text by attending over\n",
      "fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather\n",
      "distributed representations, which makes the memory both (i) human-readable, lending a form of\n",
      "interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s\n",
      "memory by editing the document index. This approach has also been used in knowledge-intensive\n",
      "dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\n",
      "rather than end-to-end learnt retrieval [9].\n",
      "Retrieve-and-Edit approaches\n",
      "correct text more often than BART. Later, we also show that RAG generations are more diverse than\n",
      "BART generations (see §4.5).\n",
      "4.3\n",
      "Jeopardy Question Generation\n",
      "Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\n",
      "with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\n",
      "pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\n",
      "than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and\n",
      "BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on\n",
      "the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more\n",
      "speciﬁc by a large margin. Table 3 shows typical generations from each model.\n",
      "Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform\n",
      "documents for questions that are less likely to beneﬁt from retrieval, suggesting that null document\n",
      "mechanisms may not be necessary for RAG.\n",
      "G\n",
      "Parameters\n",
      "Our RAG models contain the trainable parameters for the BERT-base query and document encoder of\n",
      "DPR, with 110M parameters each (although we do not train the document encoder ourselves) and\n",
      "406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\n",
      "18\n",
      "2\n",
      "Methods\n",
      "We explore RAG models, which use the input sequence x to retrieve text documents z and use them\n",
      "as additional context when generating the target sequence y. As shown in Figure 1, our models\n",
      "leverage two components: (i) a retriever pη(z|x) with parameters η that returns (top-K truncated)\n",
      "distributions over text passages given a query x and (ii) a generator pθ(yi|x, z, y1:i−1) parametrized\n",
      "1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\n",
      "ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/\n",
      "examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\n",
      "2\n",
      "The Divine\n",
      "Comedy\n",
      "BART\n",
      "*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\n",
      "RAG-T Dante’s \"Inferno\" is the ﬁrst part of this epic poem\n",
      "RAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"\n",
      "For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [35]\n",
      "to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\n",
      "within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\n",
      "We also analyze whether documents retrieved by RAG correspond to documents annotated as gold\n",
      "evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved\n",
      "by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article\n",
      "in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n",
      "4.5\n",
      "Additional Results\n",
      "Generation Diversity\n"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/how_to/ensemble_retriever\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "vectorstore_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "keyword_retriever = BM25Retriever.from_documents(recursive_chunks)\n",
    "keyword_retriever.k =  5\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[vectorstore_retriever, keyword_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "relevance_docs = ensemble_retriever.invoke(query)\n",
    "\n",
    "for d in relevance_docs:\n",
    "    print(d.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b35c99c-ecc4-4878-be35-4e96ecf1f34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n",
      "4.5\n",
      "Additional Results\n",
      "Generation Diversity\n",
      "Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
      "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n",
      "[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\n",
      "total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are\n",
      "more diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing\n",
      "any diversity-promoting decoding.\n",
      "Retrieval Ablations\n",
      "A key feature of RAG is learning to retrieve relevant information for the task.\n",
      "To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\n",
      "during training. As shown in Table 6, learned retrieval improves results for all tasks.\n",
      "decisions and updating their world knowledge remain open research problems. Pre-\n",
      "trained models with a differentiable access mechanism to explicit non-parametric\n",
      "memory have so far been only investigated for extractive downstream tasks. We\n",
      "explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\n",
      "(RAG) — models which combine pre-trained parametric and non-parametric mem-\n",
      "ory for language generation. We introduce RAG models where the parametric\n",
      "memory is a pre-trained seq2seq model and the non-parametric memory is a dense\n",
      "vector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\n",
      "pare two RAG formulations, one which conditions on the same retrieved passages\n",
      "across the whole generated sequence, and another which can use different passages\n",
      "per token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\n",
      "intensive NLP tasks and set the state of the art on three open domain QA tasks,\n",
      "two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way\n",
      "(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\n",
      "4\n",
      "Results\n",
      "4.1\n",
      "Open-domain Question Answering\n",
      "Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\n",
      "tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\n",
      "the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of\n",
      "\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\n",
      "without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s\n",
      "retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions\n",
      "and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-\n"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/integrations/document_transformers/cross_encoder_reranker/\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "rerank_model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "compressor = CrossEncoderReranker(model = rerank_model, top_n = 3)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor = compressor, \n",
    "    base_retriever = ensemble_retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(query)\n",
    "\n",
    "for d in compressed_docs:\n",
    "    print(d.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3433f435-6427-4548-821c-8bfb398cc2de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
